---
title: "Our Projects"
permalink: /projects/
excerpt: "A List of Projects"
layout: categories
comments: false
---

---

<span style="color:Salmon"> Click the titles to see the details </span>

<details>
  <summary> <span style="font-size: 20px"> (2021.04~2021.11)<br>　　다관절 로봇의 물리적 지능을 위한 교시학습 인공지능기술개발 </span> </summary>
  
  <div markdown="1">
<span style="font-size: 13px"> This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)</span>
   </div>  
  
<div style="border: 1px solid black; padding: 10px; border-color: #CCEEFF; background-color: #CCEEFF;"> 
  <br>
    <span style="font-size: 30px; color:green"> 　연구목표 </span><br>
    <div style="border: 1px solid black; padding: 10px; border-color: LightGray; background-color:rgba(211, 211, 211, 0.2);"> 
      <span style="font-size: 13px">
        ■ 　로봇 매니퓰레이터의 Task 적용을 위한 <span style="color:salmon"><b>힘/위치 궤적 교시학습</b></span> 시스템 구축 <br>
         ■ 　직접 교시를 통한 <span style="color:salmon"><b>사용자 의도 파악(힘/위치)</b></span>이 가능한 교시학습 알고리즘 개발 <br>
         ■ 　7축 다관절 로봇을 활용한 <span style="color:salmon"><b>실제 작업</b></span>에 대한 모방학습 알고리즘 적용 및 검증
        </span>
    </div>
    <br>
    <br>
    <span style="font-size: 30px; color:green"> 　연구내용 </span><br>
    <div style="border: 1px solid black; padding: 10px; border-color: LightGray; background-color:rgba(211, 211, 211, 0.2);"> 
      <span style="font-size: 13px">
   ■ 　기존의 물리적 지능을 위한 교시학습에 대한 방법 조사 및 분석<br>
   ■ 　Task 적용 힘/위치 제어시스템을 구축 <br>
   ■ 　7축 다관절 로봇을 위한 직접 교시 환경을 구축<br>
   ■ 　다관절 로봇의 힘과 위치 정보를 이용한 교시학습 알고리즘 개발<br>
   ■ 　주어진 Task에 대한 모방학습 알고리즘 적용<br>
   ■ 　적용된 모방학습 알고리즘 성능 평가
        </span>
    </div>
<br>

**Keywords**: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
  
  </div>
</details>
    
---

<details>
  <summary> <span style="font-size: 20px"> (2020.04~2020.11)<br>　　다관절 로봇을 위한 인공지능 모방학습 기술 개발 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
---

<details>
  <summary> <span style="font-size: 20px"> (2019.04~2019.11)<br>　　로봇의 센서리스 기반 외력 추정 및 임피던스 제어 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
---

<details>
  <summary> <span style="font-size: 20px"> (2019.03~2019.11)<br>　　지능형 이동로봇 추적제어 알고리즘 구현 및 실험 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
---

<details>
  <summary> <span style="font-size: 20px"> (2018.06~2018.11)<br>　　2018 R-BIZ challenge  터틀봇3 오토레이스 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>


***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
---

<details>
  <summary> <span style="font-size: 20px"> (2018.03~2018.11)<br>　　다중주기 센서융합 기반 이동체 실시간 예측 제어 연구 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:#3399ff">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Research on Unmanned Aerial Vehicles has been actively conducted in recent years. In particular, the UAV to explore an unknown, GNSS-denied environment is required, but the self-localization method, such as Visual Inertial Odometry, is mandatory to operate it. Considering the payload and the operating time of the UAV, lightweight and low-power consuming cameras and IMU are preferred, and even Object Detection and 3D Mapping can be obtained using a RGB-D camera. In this work, we developed a 3D Mapping system including object positions in an unknown and GNSS-denied environment for the UAV with a RGB-D camera. The system is demonstrated in Gazebo simulator, and the quantitative and qualitative results are obtained.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
---

<details>
  <summary> <span style="font-size: 20px"> (2016.11~2019.10)<br>　영상검출오류에 강인한 예측 비주얼 서보잉 기법 개발 </span> </summary>
  <div markdown="1">
This project was conducted at <span style="color:grin">Cyber Physical System Control Lab in Kyungpook National University</span> : [Link](http://control.knu.ac.kr/)
  </div>  
<br>
<p style="font-size:1rem;font-weight:400" onContextMenu="return false;" onselectstart="return false" ondragstart="return false">
　Visual servo control refers to the use of computer vision data to control the motion of a robot. The vision data may be acquired from a camera that is mounted directly on a robot manipulator or on a mobile robot, in which case motion of the robot induces camera motion, or the camera can be fixed in the workspace so that it can observe the robot motion from a stationary configuration. Other configurations can be considered such as, for instance, several cameras mounted on pan-tilt heads observing the robot motion.</p>

<br>

***Keywords***: Autonomous Vehicle, Visual servoing, Multi-Channel LiDAR , Sensor Fusion

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/j8nnk5R37XU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
</p>
</details>
    
---
